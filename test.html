<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Face Landmark Detection with MediaPipe and Three.js</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <style>
      body {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
        margin: 0;
        background-color: #f0f0f0;
      }
      .canvasContainer {
        position: relative;
        width: 640px;
        height: 480px;
      }
      video {
        display: none;
      }
      canvas {
        position: absolute;
        top: 0;
        left: 0;
      }
      #outputCanvas {
        z-index: 1;
      }
      #threeCanvas {
        z-index: 2;
      }
    </style>
  </head>
  <body>
    <video id="videoInput" playsinline></video>
    <div class="canvasContainer">
      <canvas id="outputCanvas" width="640" height="480"></canvas>
      <canvas id="threeCanvas" width="640" height="480"></canvas>
    </div>

    <script>
      const videoElement = document.getElementById("videoInput");
      const outputCanvas = document.getElementById("outputCanvas");
      const outputCtx = outputCanvas.getContext("2d");
      const threeCanvas = document.getElementById("threeCanvas");

      let renderer, scene, camera;
      let faceLandmarks = [];

      // Function to start the video stream
      async function startVideo() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            video: { facingMode: "user" },
          });
          videoElement.srcObject = stream;
          await videoElement.play();
        } catch (err) {
          console.error("Error accessing the camera: ", err);
        }
      }

      startVideo();

      const faceMesh = new FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });

      faceMesh.onResults(onResults);

      // Three.js setup
      try {
        renderer = new THREE.WebGLRenderer({
          canvas: threeCanvas,
          alpha: true,
          antialias: true,
        });
        renderer.setSize(threeCanvas.width, threeCanvas.height);
        renderer.autoClear = false;
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(
          50,
          threeCanvas.width / threeCanvas.height,
          0.1,
          1000
        );
        camera.position.z = 500;
      } catch (e) {
        console.error(
          "WebGL not supported, falling back on experimental-webgl",
          e
        );
        renderer = null;
        alert(
          "WebGL not supported, please update your browser or GPU drivers."
        );
      }

      function createFaceMesh(landmarks) {
        const eyebrowLeftVertex = [
          [293, 300, 276],
          [293, 283, 276],
          [293, 283, 282],
          [293, 334, 282],
          [296, 334, 282],
          [296, 295, 282],
          [336, 296, 295],
          [336, 285, 295],
        ];

        const eyebrowRightVertex = [
          [63, 70, 46],
          [63, 53, 46],
          [63, 53, 52],
          [63, 105, 52],
          [66, 65, 52],
          [66, 105, 52],
          [107, 55, 65],
          [107, 65, 66],
        ];

        const facesVertex = [...eyebrowRightVertex, ...eyebrowLeftVertex];

        const vertices = [];
        landmarks.forEach((lm) => {
          vertices.push(
            (1 - lm.x) * outputCanvas.width - outputCanvas.width / 2, // Centering x
            -lm.y * outputCanvas.height + outputCanvas.height / 2, // Centering y and flipping
            0 // Adjust depth if necessary
          );
        });

        const indices = [];
        facesVertex.forEach((face) => {
          indices.push(...face);
        });

        const geometry = new THREE.BufferGeometry();

        geometry.setAttribute(
          "position",
          new THREE.Float32BufferAttribute(vertices, 3)
        );

        geometry.setIndex(indices);

        const material = new THREE.MeshStandardMaterial({
          color: 0xff0000,
          roughness: 0.0, // Adjust roughness value
          metalness: 0.5, // Optionally adjust metalness
          side: THREE.DoubleSide,
          transparent: true,
          opacity: 0.6,
          flatShading: true,
          wireframe: false,
          depthTest: true,
          depthWrite: true,
          emissive: 0x000000,
        });

        const mesh = new THREE.Mesh(geometry, material);

        return mesh;
      }

      function onResults(results) {
        if (
          !results.multiFaceLandmarks ||
          results.multiFaceLandmarks.length === 0
        ) {
          renderer.render(scene, camera);
          return;
        }

        outputCtx.save();
        outputCtx.clearRect(0, 0, outputCanvas.width, outputCanvas.height);
        outputCtx.translate(outputCanvas.width, 0);
        outputCtx.scale(-1, 1);
        outputCtx.drawImage(
          results.image,
          0,
          0,
          outputCanvas.width,
          outputCanvas.height
        );

        scene.clear(); // Clear previous meshes

        results.multiFaceLandmarks.forEach((landmarks) => {
          const faceMesh = createFaceMesh(landmarks);
          scene.add(faceMesh);
        });

        renderer.render(scene, camera);
        outputCtx.restore();
      }

      const cameraStream = new Camera(videoElement, {
        onFrame: async () => {
          await faceMesh.send({ image: videoElement });
        },
        width: 640,
        height: 480,
      });

      cameraStream.start();
    </script>
  </body>
</html>
