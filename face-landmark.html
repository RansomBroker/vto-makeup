<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Face Landmark Detection with MediaPipe and Three.js</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <style>
      body {
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
        margin: 0;
        background-color: #f0f0f0;
      }
      .canvasContainer {
        position: relative;
        width: 640px;
        height: 480px;
      }
      video {
        display: none;
      }
      canvas {
        position: absolute;
        top: 0;
        left: 0;
      }
      #outputCanvas {
        z-index: 1;
      }
      #threeCanvas {
        z-index: 2;
      }
    </style>
  </head>
  <body>
    <video id="videoInput" playsinline></video>
    <div class="canvasContainer">
      <canvas id="outputCanvas" width="640" height="480"></canvas>
      <canvas id="threeCanvas" width="640" height="480"></canvas>
    </div>

    <script>
      const videoElement = document.getElementById("videoInput");
      const outputCanvas = document.getElementById("outputCanvas");
      const outputCtx = outputCanvas.getContext("2d");
      const threeCanvas = document.getElementById("threeCanvas");

      let renderer, scene, camera;
      let faceLandmarks = [];

      // Function to start the video stream
      async function startVideo() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            video: { facingMode: "user" },
          });
          videoElement.srcObject = stream;
          await videoElement.play();
        } catch (err) {
          console.error("Error accessing the camera: ", err);
        }
      }

      startVideo();

      const faceMesh = new FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.7, // Increased confidence
        minTrackingConfidence: 0.7, // Increased confidence
      });

      faceMesh.onResults(onResults);

      // Three.js setup
      try {
        renderer = new THREE.WebGLRenderer({
          canvas: threeCanvas,
          alpha: true,
        });
        renderer.setSize(threeCanvas.width, threeCanvas.height);
        renderer.autoClear = false;
        scene = new THREE.Scene();
        camera = new THREE.PerspectiveCamera(
          50,
          threeCanvas.width / threeCanvas.height,
          0.1,
          1000
        );
        camera.position.z = 500;

        // Add lighting
        const light = new THREE.AmbientLight(0xffffff, 1);
        scene.add(light);
      } catch (e) {
        console.error(
          "WebGL not supported, falling back on experimental-webgl",
          e
        );
        renderer = null;
        alert(
          "WebGL not supported, please update your browser or GPU drivers."
        );
      }

      let cubes = [];

      // Function to create a cube
      function createCube(x, y, z) {
        const geometry = new THREE.BoxGeometry(5, 5, 5);
        const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });
        const cube = new THREE.Mesh(geometry, material);
        cube.position.set(x, y, z);
        scene.add(cube);
        return cube;
      }

      function onResults(results) {
        if (
          !results.multiFaceLandmarks ||
          results.multiFaceLandmarks.length === 0
        ) {
          cubes.forEach((cube) => {
            scene.remove(cube);
          });
          cubes = [];
          renderer.render(scene, camera);
          return;
        }

        outputCtx.save();
        outputCtx.clearRect(0, 0, outputCanvas.width, outputCanvas.height);
        outputCtx.translate(outputCanvas.width, 0);
        outputCtx.scale(-1, 1);
        outputCtx.drawImage(
          results.image,
          0,
          0,
          outputCanvas.width,
          outputCanvas.height
        );

        results.multiFaceLandmarks.forEach((landmarks) => {
          if (cubes.length === 0) {
            landmarks.forEach((landmark) => {
              const x =
                (1 - landmark.x) * outputCanvas.width - outputCanvas.width / 2;
              const y =
                -landmark.y * outputCanvas.height + outputCanvas.height / 2;
              const z = -landmark.z * 100;
              cubes.push(createCube(x, y, z));
            });
          } else {
            landmarks.forEach((landmark, index) => {
              const x =
                (1 - landmark.x) * outputCanvas.width - outputCanvas.width / 2;
              const y =
                -landmark.y * outputCanvas.height + outputCanvas.height / 2;
              const z = -landmark.z * 100;
              cubes[index].position.set(x, y, z);
            });
          }
        });

        renderer.render(scene, camera);
        outputCtx.restore();
      }

      const cameraStream = new Camera(videoElement, {
        onFrame: async () => {
          await faceMesh.send({ image: videoElement });
        },
        width: 640,
        height: 480,
      });

      cameraStream.start();
    </script>
  </body>
</html>
